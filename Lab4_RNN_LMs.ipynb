{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab4_RNN_LMs.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6J8ZJaIRoryP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQCuM5Y3qfMA"
      },
      "source": [
        "# Working with Sequence Data using RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo7dTKe_qb5h"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rocBq67rExT"
      },
      "source": [
        "! wget https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/raw/master/data/ptb.train.txt\n",
        "! wget https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/raw/master/data/ptb.valid.txt\n",
        "! wget https://github.com/townie/PTB-dataset-from-Tomas-Mikolov-s-webpage/raw/master/data/ptb.test.txt\n",
        "\n",
        "! sed -e 's/<unk>//g' ptb.train.txt > ptb.train.nounk.txt\n",
        "! sed -e 's/<unk>//g' ptb.valid.txt > ptb.valid.nounk.txt\n",
        "! sed -e 's/<unk>//g' ptb.test.txt > ptb.test.nounk.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVGNncf6iz89"
      },
      "source": [
        "## RNNCell vs RNN\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a little unique because they are a recurrent neural network module: they get reused multiple times in a single forward pass over different time steps. Because of this, there are two implementations of the same core logic in torch:\n",
        "\n",
        "1. `RNNCell` which is a basic implementation that runs for a single time step\n",
        "2. `RNN` which includes additional components such as multiple layers, and potentially having a more efficient CUDA implementation as well.\n",
        "\n",
        "(You will see a similar convention for other RNN models, e.g. `LSTMCell` and `LSTM`.)\n",
        "\n",
        "You will likely primarily use the `RNN` implementation, but it is worth understanding how the two interact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J8ZJaIRoryP"
      },
      "source": [
        "### Full Explanation (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8eADvsFfKhW"
      },
      "source": [
        "<img src=\"https://i.stack.imgur.com/SjnTl.png\" style=\"width:80%\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YplIHiv4jzPp"
      },
      "source": [
        "We can start by breaking down what `RNNCell` and `RNN` are.\n",
        "\n",
        "To recap, we expect an `RNNCell` to take in two inputs (a) the input for that time step, $x_t$, and (b) the current hidden state $h_t$, and outputs $h_{t+1}$, the hidden state for the next time step.\n",
        "\n",
        "(Note: the $t$ notation may differ across materials, but generally they refer to the same thing.)\n",
        "\n",
        "$$ h_{t+1} = \\text{RNNCell}(x_t, h_t) $$\n",
        "\n",
        "An `RNN` primarily incorporates two additional things:\n",
        "\n",
        "1. Processing across time steps\n",
        "2. Processing across layers\n",
        "\n",
        "Let us first consider a single layer `RNN`. We can describe it as the following:\n",
        "\n",
        "\\begin{align}\n",
        "  \\text{Output}, \\text{FinalHiddenState} &= \\text{RNN}(\n",
        "    [x_{0} \\cdots x_{T}],\n",
        "    h_{0}\n",
        "  ) \\\\\n",
        "  \\text{where} \\\\\n",
        "  \\text{Output} &= [h_{1} \\cdots h_{T+1}] \\\\\n",
        "  \\text{FinalHiddenStates} &= [h_{T+1}]\n",
        "\\end{align}\n",
        "\n",
        "You will need to provide the whole time sequence of $x_t$ at once, but you only need to provide $h_0$. The `RNN` implementation will automatically feed the resulting $h_1$ back into the model as input for the next time step, and repeat until it is done computing the whole sequence.\n",
        "\n",
        "`RNN` actually returns two outputs. One for the \"output\" of the RNN, and another only consisting of the final hidden state. This may seem redundant, but makes sense when we consider a multi-layer RNN.\n",
        "\n",
        "Let us modify the notation slightly. Let $h_{t,l}$ refer to the hidden state at time step $t$ and layer $l$. Let's assume as have layers going from $0$ to $L$.Hence, we have as inputs $x_t$ and $h_{0, 0}$. Then, `RNN` does the following:\n",
        "\n",
        "\\begin{align}\n",
        "  \\text{Output}, \\text{FinalHiddenState} &= \\text{RNN}(\n",
        "    [x_{0} \\cdots x_{T}],\n",
        "    h_{0,0}\n",
        "  ) \\\\\n",
        "  \\text{where} \\\\\n",
        "  \\text{Output} &= [h_{1,L} \\cdots h_{T+1, L}] \\\\\n",
        "  \\text{FinalHiddenStates} &= [h_{T+1,0} \\cdots h_{T+1,L}]\n",
        "\\end{align}\n",
        "\n",
        "Now we see the difference: \"Output\" contains the hidden states for the final layer for all time steps, while \"FinalHiddenState\" contains the hidden states for the final time step for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_ia4SKXn93z"
      },
      "source": [
        "Now, let's see this in code.\n",
        "\n",
        "First, let's do some setup. We'll define some hyperparameters, and define `rnn_cell` and `rnn`.\n",
        "\n",
        "**Note**: Because of the way RNNs are computed by iterating over the time steps, it is actually more efficient to have time be the first dimension of the input, rather than the batch size. For simplicity, we are going to use the slightly more inefficient version, keeping batch size first, and using the `batch_first=True` argument when creating our `RNN`. If you want to switch over, be sure to update both the `RNN` initialization arguments as well as the input shapes (which can be modified via a simple transpose)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzJUaFsZovtk"
      },
      "source": [
        "### Code `RNNCell` vs `RNN`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E2iOBTrrChr"
      },
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import trange, tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbYEvAdP7AbQ"
      },
      "source": [
        "# Hyperparameters and setup\n",
        "rng = np.random.default_rng(1111)\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "hidden_dim = 4\n",
        "input_dim = 8\n",
        "\n",
        "# Create our RNN implementations\n",
        "rnn_cell = nn.RNNCell(\n",
        "    input_size=input_dim, \n",
        "    hidden_size=hidden_dim,\n",
        "    nonlinearity=\"tanh\",\n",
        ")\n",
        "rnn = nn.RNN(\n",
        "    input_size=input_dim, \n",
        "    hidden_size=hidden_dim,\n",
        "    nonlinearity=\"tanh\",\n",
        "    num_layers=1,\n",
        "    batch_first=True,  # <-- Note this!\n",
        ")\n",
        "\n",
        "# Initialize some inputs\n",
        "inputs = torch.from_numpy(rng.random(\n",
        "    size=(batch_size, seq_len, input_dim),\n",
        ")).float()\n",
        "initial_hidden_state = torch.from_numpy(rng.random(\n",
        "    size=(batch_size, hidden_dim),\n",
        ")).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa742lt8o8SJ"
      },
      "source": [
        "We can see that they have identified parameters (albeit with different names)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRoQ1075o9j8"
      },
      "source": [
        "print(\"RNNCell parameters\")\n",
        "for name, params in rnn_cell.named_parameters():\n",
        "    print(name, tuple(params.shape))\n",
        "\n",
        "print()\n",
        "print(\"RNN parameters\")\n",
        "for name, params in rnn.named_parameters():\n",
        "    print(name, tuple(params.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBB_WE9-ouk2"
      },
      "source": [
        "Because we want to directly compare the two implementations, we want to ensure that they have the same weights. Let's do that be assigning them to be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foFZzuqhY9UZ"
      },
      "source": [
        "# Set to the exact same weights\n",
        "rnn.weight_ih_l0 = rnn_cell.weight_ih\n",
        "rnn.weight_hh_l0 = rnn_cell.weight_hh\n",
        "rnn.bias_ih_l0 = rnn_cell.bias_ih\n",
        "rnn.bias_hh_l0 = rnn_cell.bias_hh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4LJivwQpKgg"
      },
      "source": [
        "Let's do the RNNCell forward pass first. We supply $x_t$ and $h_t$ at each time step, iterating over time manually with a loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQhmXQynYuzN"
      },
      "source": [
        "# RNNCell forward pass\n",
        "with torch.no_grad():\n",
        "    hidden_state = initial_hidden_state\n",
        "    for time_step in range(seq_len):\n",
        "        hidden_state = rnn_cell(\n",
        "            inputs[:, time_step],\n",
        "            hidden_state,\n",
        "        )\n",
        "    rnn_cell_final_hidden_state = hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUO99s9_pcx0"
      },
      "source": [
        "Let's do the equivalent for `RNN`, where we supply `[x_0 \\cdots x_T]` and `h_0`. The corresponding outputs are computed all at once. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N2A8MlPYu3e"
      },
      "source": [
        "# RNN forward pass\n",
        "with torch.no_grad():\n",
        "    rnn_out, rnn_final_hidden_states = rnn(\n",
        "        inputs,\n",
        "        initial_hidden_state.unsqueeze(0),  # <-- add a [1] dimension, corresponding to the layer\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXVkcDsXp3Si"
      },
      "source": [
        "Now, let's look at the dimensions of our output tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU3YwEOKYsBm"
      },
      "source": [
        "# RNNCell final hidden state: [batch_size, hidden_dim]\n",
        "print(rnn_cell_final_hidden_state.shape)\n",
        "\n",
        "# RNN final output: [batch_size, seq_len, hidden_dim]\n",
        "print(rnn_out.shape)\n",
        "\n",
        "# RNN final hidden states: [num_layers, batch_size, hidden_dim]\n",
        "print(rnn_final_hidden_states.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgdmoSrUp89x"
      },
      "source": [
        "We see that we get the same results with a manual loop over `RNNCell`, or with an `RNN`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghcaOCpbaN30"
      },
      "source": [
        "# RNN final output is the last hidden state of the last layer of the RNN\n",
        "assert torch.allclose(\n",
        "    rnn_out[:, -1],\n",
        "    rnn_final_hidden_states[0],\n",
        ")\n",
        "\n",
        "# RNNCell final hidden state is the last hidden state of the last layer of the RNN\n",
        "assert torch.allclose(\n",
        "    rnn_cell_final_hidden_state,\n",
        "    rnn_out[:, -1],\n",
        ")\n",
        "assert torch.allclose(\n",
        "    rnn_cell_final_hidden_state,\n",
        "    rnn_final_hidden_states[0],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d74XxAwqHqd"
      },
      "source": [
        "In short, `RNN` provides a much more convenient (and potentially more efficient) implementation of using `RNNCell`. We covered the single layer example, but you can see how it will be even more convenient for multi-layer RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb867eQiq6aM"
      },
      "source": [
        "## Training an RNN-LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6eMNihLrOZ5"
      },
      "source": [
        "In this section, we will train an RNN language model.\n",
        "\n",
        "Our preprocessing is quite similar to what we did in the previous lab. The highlights are:\n",
        "\n",
        "1. Creating a vocabulary based on the training set\n",
        "2. Adding the `<pad>` (id=0) and `<unk>` (id=1) tokens to our vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMcX97xq80Y"
      },
      "source": [
        "import itertools\n",
        "import joblib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import trange, tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wHnfswCJSvt"
      },
      "source": [
        "# We can use the unk versions actually, but for fair comparison we won't\n",
        "TRAIN_PATH = \"ptb.train.nounk.txt\"\n",
        "VAL_PATH = \"ptb.valid.nounk.txt\"\n",
        "TEST_PATH = \"ptb.test.nounk.txt\"\n",
        "PAD = \"<pad>\"\n",
        "PAD_IDX = 0\n",
        "UNK = \"<unk>\"\n",
        "UNK_IDX = 1\n",
        "BATCH_SIZE = 128\n",
        "MAX_SEQ_LENGTH = 300\n",
        "DEVICE = torch.device(\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZyHdYeIJSzv"
      },
      "source": [
        "def load_tokens(path):\n",
        "    \"\"\"Read file and generate list of list of tokens.\"\"\" \n",
        "    with open(path, \"r\") as f:\n",
        "        tokens_list = []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line: \n",
        "                continue\n",
        "            tokens_list.append([\"<bos>\"] + line.split() + [\"<eos>\"])\n",
        "    return tokens_list\n",
        "\n",
        "def get_tokenized_arr(tokens_list, tok2idx):\n",
        "    \"\"\"Convert list of list of tokens to list of list of token IDs, with UNK replacement.\"\"\"\n",
        "    example_rows = []\n",
        "    for tokens in tokens_list:\n",
        "        row = [tok2idx.get(token, tok2idx[UNK]) for token in tokens]\n",
        "        example_rows.append(row)\n",
        "    return example_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGogBvHvr_2Z"
      },
      "source": [
        "For our `LMDataset`, we will actually make things simple, returning just the array of (truncated and padded) token IDs. We do not distinguish between input and labels yet. We will see why shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0vyOv80r-5K"
      },
      "source": [
        "class LMDataset(Dataset):\n",
        "   \n",
        "    def __init__(self, tokenized_arr, max_seq_length=MAX_SEQ_LENGTH):\n",
        "        self.tokenized_arr = tokenized_arr\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_arr)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        # Truncate to max_seq_length\n",
        "        x = self.tokenized_arr[key][:self.max_seq_length]\n",
        "\n",
        "        # Pad\n",
        "        arr = np.pad(x, (0, self.max_seq_length - len(x)), constant_values=PAD_IDX)\n",
        "        assert len(arr) == self.max_seq_length\n",
        "        return {\n",
        "            \"arr\": arr,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3nqBUyFseET"
      },
      "source": [
        "Our data pipeline should be straightforward and resemble what we have seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPDlYp2hJSaL"
      },
      "source": [
        "# Load raw token data\n",
        "train_tokens = load_tokens(TRAIN_PATH)\n",
        "val_tokens = load_tokens(VAL_PATH)\n",
        "test_tokens = load_tokens(TEST_PATH)\n",
        "\n",
        "# Build vocab\n",
        "vocab = [PAD] + [UNK] + sorted(list({\n",
        "    token \n",
        "    for token_line in train_tokens \n",
        "    for token in token_line\n",
        "    if token != UNK\n",
        "}))\n",
        "tok2idx = {vocab: i for i, vocab in enumerate(vocab)}\n",
        "idx2tok = dict(enumerate(vocab))\n",
        "\n",
        "# Create raw data arrays\n",
        "train_arr = get_tokenized_arr(train_tokens, tok2idx)\n",
        "val_arr = get_tokenized_arr(val_tokens, tok2idx)\n",
        "test_arr = get_tokenized_arr(test_tokens, tok2idx)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LMDataset(train_arr)\n",
        "val_dataset = LMDataset(val_arr)\n",
        "test_dataset = LMDataset(test_arr)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBfyzGrds2oo"
      },
      "source": [
        "Now, given a batch from our dataloader, we will write a function to create our inputs and labels.\n",
        "\n",
        "Remember that for model, we want to tackle the following prediction problem:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cppZ7lr0vVte"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73knpjdqvXgJ"
      },
      "source": [
        "We see that language modeling has a nice property where the labels are simply a 1-shifted version of our input. As long as we ensure that the model has no way of cheating and making use of future labels, it will not be an issue.\n",
        "\n",
        "In the case of RNNs, there is no cheating because the inputs are processed sequentially. We will see in the case of later models like Transformers that we will need special handling to prevent this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5njwL8xJyGq"
      },
      "source": [
        "def format_batch(batch):\n",
        "    \"\"\"Take a batch from the dataloader and create inputs for our models/training.\"\"\"\n",
        "    arr = batch[\"arr\"].to(DEVICE)\n",
        "    return {\n",
        "        # Input: Drop last token\n",
        "        \"inputs\": arr[:, :-1],\n",
        "        # Label: Drop first token\n",
        "        \"labels\": arr[:, 1:],\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwTqOgkUvzft"
      },
      "source": [
        "Now, let's make define our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMdlUQbCNKTI"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, num_layers, dropout_p=0.5):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=emb_dim,\n",
        "            hidden_size=emb_dim,\n",
        "            num_layers=num_layers,\n",
        "            nonlinearity=\"tanh\",\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.out_embed = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "        # Share weights between embeddings\n",
        "        self.out_embed.weight = self.in_embed.weight\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.in_embed(x)\n",
        "        # h_0 is all zeros if it is not provided\n",
        "        out, hidden = self.rnn(out)\n",
        "        out = self.out_embed(self.dropout(out))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOQ8lwQwv3LY"
      },
      "source": [
        "We will also write our evaluation loop and train step implementations similar to before. In language modeling, it is more common to think in terms of training steps rather than training epochs, since often we have abundant amounts of training data. In some cases, some big models never even finish 1 epoch of training!\n",
        "\n",
        "**Important**: Pay close attention how how the cross entropy loss function is used. As discussed before, we can either use a `nn.CrossEntropyLoss` object or a `F.cross_entropy` function. Let's focus on this part from `train_step`:\n",
        "\n",
        "```python\n",
        "flattened_logits = logits.reshape(-1, len(vocab))\n",
        "flattened_labels = batch[\"labels\"].reshape(-1)\n",
        "loss = F.cross_entropy(\n",
        "    flattened_logits,\n",
        "    flattened_labels,\n",
        "    reduction='mean',\n",
        "    ignore_index=PAD_IDX,  # <-- This is important!\n",
        ")\n",
        "```\n",
        "\n",
        "First, we see that we do some reshaping of the logits and labels. Remember that the logits should have shape `[bs, seq_len, vocab]` while labels have shape `[bs, seq_len]`. Because `F.cross_entropy` expects logits of shape `[N, classes]` and `[N]`, we flatten the first two dimensions of both logits and labels.\n",
        "\n",
        "Secondly, note that we have `ignore_index=PAD_IDX`. Remember that we have padded all of our inputs to be of the same length. If we do not add this `ignore_index` here, our loss function will also compute losses against the padding tokens! By supplying this argument, the `F.cross_entropy` function will ignore all examples where the label ID is `PAD_IDX`. When it computes the mean loss over all tokens, it will also take that into consideration for the denominator.\n",
        "\n",
        "You can also see it as doing the following:\n",
        "\n",
        "```python\n",
        "raw_loss = F.cross_entropy(\n",
        "    flattened_logits,\n",
        "    flattened_labels,\n",
        "    reduction='none',\n",
        ")\n",
        "is_valid = (flattened_labels != PAD_IDX).bool()\n",
        "loss = raw_loss[is_valid].sum() / is_valid.sum()\n",
        "```\n",
        "\n",
        "You will see in our `eval_loop` that we have a slightly more complex handling because we also want to compute our per-token accuracy. (In practice, token accuracy is not often used to measure language model performance, but it is a good exercise to see what we do to handle it.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGDHbgEVNZ-y"
      },
      "source": [
        "@torch.no_grad()\n",
        "def eval_loop(model, dataloader, num_batches=None):\n",
        "    \"\"\"Run validation phase.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Keeping track of metrics\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0.0\n",
        "    total_count = 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(dataloader)\n",
        "    for raw_batch in tqdm(dataloader, desc=\"Val\", total=num_batches):\n",
        "        batch = format_batch(raw_batch)\n",
        "        logits = model(batch[\"inputs\"])\n",
        "        flattened_logits = logits.reshape(-1, len(vocab))\n",
        "        flattened_labels = batch[\"labels\"].reshape(-1)\n",
        "        loss = F.cross_entropy(\n",
        "            flattened_logits,\n",
        "            flattened_labels,\n",
        "            reduction='sum',\n",
        "            ignore_index=PAD_IDX,  # <-- This is important!\n",
        "        )\n",
        "\n",
        "        # Only count non-padding tokens\n",
        "        # (Same idea as ignore_index=PAD_IDX above)\n",
        "        valid_indices = flattened_labels != PAD_IDX\n",
        "        valid_count = valid_indices.sum().item()\n",
        "        preds = flattened_logits.argmax(-1)\n",
        "        raw_correct_preds = (preds == flattened_labels)\n",
        "        correct_preds = (raw_correct_preds & valid_indices).sum()\n",
        "\n",
        "        # Keeping track of metrics\n",
        "        total_loss += loss.item()\n",
        "        total_correct += correct_preds.item()\n",
        "        total_count += valid_count\n",
        "    return {\n",
        "        \"loss\": total_loss / total_count,\n",
        "        \"acc\": total_correct / total_count,\n",
        "    }\n",
        "\n",
        "def train_step(optimizer, model, raw_batch):\n",
        "    \"\"\"Run a single train step.\"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    batch = format_batch(raw_batch)\n",
        "    logits = model(batch[\"inputs\"])\n",
        "    flattened_logits = logits.reshape(-1, len(vocab))\n",
        "    flattened_labels = batch[\"labels\"].reshape(-1)\n",
        "    loss = F.cross_entropy(\n",
        "        flattened_logits,\n",
        "        flattened_labels,\n",
        "        reduction='mean',\n",
        "        ignore_index=PAD_IDX,  # <-- This is important!\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rllw4jypydKl"
      },
      "source": [
        "Instead of accuracy, we often use perplexity to measure the performance of language models. Perplexity is defined as\n",
        "\n",
        "$$ \\text{PPL}(x) = \\exp \\left(-\\frac{1}{\\sum_{n=1}^NT^n}\\sum_{n=1}^T \\sum_{t=1}^{T^n} \\log p(w_t^n | w^n_{<t})\\right)$$\n",
        "\n",
        "Remember that our loss corresponds to the (average) negative loglikelihood of our data (over all sequences in the corpus). You should then be able to see that perplexity is simply the exponentiaion of our cross entropy loss.\n",
        "\n",
        "An \"intuitive\" interpretation of perplexity is, if for each token you were to line up all your model token predictions from most to least likely and go down the list until you get the correct token, how far down on the list you would have to go *on average*.\n",
        "\n",
        "Also take a look at https://huggingface.co/transformers/perplexity.html for more details about evaluating perplexity with language models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LNpMypVyfXA"
      },
      "source": [
        "def compute_perplexity(loss_val):\n",
        "    return np.exp(loss_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w71QXPs1dTx"
      },
      "source": [
        "Now that all our pieces are in place, let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLqUh9dwNZyl"
      },
      "source": [
        "rnn_model = RNNModel(vocab_size=len(vocab), num_layers=4, emb_dim=200).to(DEVICE)\n",
        "optimizer = optim.AdamW(rnn_model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCobfQXjN9dd"
      },
      "source": [
        "NUM_EPOCHS = 20\n",
        "total_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "train_loss_list = []\n",
        "for step, raw_batch in zip(range(total_steps), itertools.cycle(train_dataloader)):\n",
        "    loss_val = train_step(\n",
        "        optimizer=optimizer,\n",
        "        model=rnn_model,\n",
        "        raw_batch=raw_batch,\n",
        "    )\n",
        "    train_loss_list.append(loss_val)\n",
        "    if step % 100 == 0 and step != 0:\n",
        "        val_results = eval_loop(\n",
        "            model=rnn_model,\n",
        "            dataloader=val_dataloader,\n",
        "        )\n",
        "        print(\"Step: {}/{}, train perp: {:.1f}, val perp: {:.1f}, acc: {:.3f}\".format(\n",
        "            step, \n",
        "            total_steps,\n",
        "            compute_perplexity(np.mean(train_loss_list)),\n",
        "            compute_perplexity(val_results[\"loss\"]),\n",
        "            val_results[\"acc\"])\n",
        "        )\n",
        "        train_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ1IGqOggAgL"
      },
      "source": [
        "## Comparison between LMs\n",
        "\n",
        "We have now considered three kinds of language models:\n",
        "\n",
        "- Count-based N-gram model\n",
        "- Feed-forward N-gram model\n",
        "- RNN model\n",
        "\n",
        "We will use **perplexity** as our comparison metric.\n",
        "\n",
        "To provide a fair comparison between the different models, we need to ensure that:\n",
        "\n",
        "- We use the same vocabulary across all models\n",
        "- Exactly the same tokens are evaluated (which means we need to handle padding identically in all cases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k3s8EhYgoLZ"
      },
      "source": [
        "### Count-based N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnQTVGwIgAEE"
      },
      "source": [
        "# Refer to Lab 3\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.child_nodes = {}\n",
        "\n",
        "    def has_token(self, token):\n",
        "        return token in self.child_nodes\n",
        "\n",
        "    def create_child_node(self, token):\n",
        "        self.child_nodes[token] = Node()\n",
        "\n",
        "    def increment_count(self):\n",
        "        self.count += 1\n",
        "    \n",
        "    def get_child_node(self, token):\n",
        "        if token not in self.child_nodes:\n",
        "            raise KeyError(f\"'{token}' not found in child node\")\n",
        "        return self.child_nodes[token]\n",
        "        \n",
        "\n",
        "class NgramTrie:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.root_node = Node()\n",
        "        self.vocab = set()\n",
        "    \n",
        "    def add_ngram(self, ngram):\n",
        "        if len(ngram) > self.n:\n",
        "            raise RuntimeError(f\"This NgramTrie only supports up to {self.n}-grams\")\n",
        "\n",
        "        # Update vocabulary\n",
        "        self.vocab.update(ngram)\n",
        "\n",
        "        # Update Trie\n",
        "        current_node = self.root_node\n",
        "        current_node.increment_count()\n",
        "        for token in ngram:\n",
        "            if not current_node.has_token(token):\n",
        "                current_node.create_child_node(token)\n",
        "            current_node = current_node.get_child_node(token)\n",
        "            current_node.increment_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R60Vv5LOgqPd"
      },
      "source": [
        "# We prepend 3 <pad> tokens to ensure our data is consistent with the RNN data\n",
        "def get_ngram_dataset(tokens_list, tok2idx, n):\n",
        "    ngrams_list = []\n",
        "    for tokens in tokens_list:\n",
        "        tokens = [PAD, PAD, PAD] + tokens\n",
        "        sequence = [tok2idx.get(token, tok2idx[UNK]) for token in tokens]\n",
        "        for i in range(len(sequence)-n+1):\n",
        "            sequence = sequence[:MAX_SEQ_LENGTH]\n",
        "            ngram = tuple(sequence[i:i+n])\n",
        "            ngrams_list.append(ngram)\n",
        "    return ngrams_list\n",
        "\n",
        "# We apply add-1 smoothing to ensure we never assign 0 probability to a token, \n",
        "# or run into issues with unseen N-grams\n",
        "def get_cbngram_cond_dist_add1_smoothing(trie, input_ngram):\n",
        "    current_node = trie.root_node\n",
        "    # ADD-1 SMOOTHING (initialize before for efficiency)\n",
        "    child_counts = {\n",
        "        token: 1\n",
        "        for token in trie.vocab\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Traverse trie\n",
        "        for token in input_ngram:\n",
        "            current_node = current_node.get_child_node(token)\n",
        "        # Get count for each child node\n",
        "        for token, node in current_node.child_nodes.items():\n",
        "            child_counts[token] += node.count\n",
        "    except KeyError:\n",
        "        # Unseen N-gram\n",
        "        pass\n",
        "\n",
        "    # Divide by total counts\n",
        "    total_count = sum(child_counts.values())\n",
        "    child_prob = {}\n",
        "    for token, count in child_counts.items():\n",
        "        child_prob[token] = count / total_count\n",
        "    return child_prob\n",
        "\n",
        "def get_cbngram_perplexity(trie, ngrams_list):\n",
        "    log_prob_list = []\n",
        "    for ngram in tqdm(ngrams_list):\n",
        "        input_ngram, label = ngram[:-1], ngram[-1]\n",
        "        cond_dist = get_cbngram_cond_dist_add1_smoothing(trie_4gram, input_ngram)\n",
        "        log_prob = np.log(cond_dist[label])\n",
        "        log_prob_list.append(log_prob)\n",
        "    count_based_ngram_perplexity = np.exp(-np.mean(log_prob_list))\n",
        "    return count_based_ngram_perplexity\n",
        "\n",
        "def get_cbngram_perplexity_parallel(trie, ngrams_list, n_jobs=4):\n",
        "    log_prob_list = []\n",
        "    def func(trie, sub_ngram_list):\n",
        "        sub_log_prob_list = []\n",
        "        for ngram in sub_ngram_list:\n",
        "            input_ngram, label = ngram[:-1], ngram[-1]\n",
        "            cond_dist = get_cbngram_cond_dist_add1_smoothing(trie, input_ngram)\n",
        "            log_prob = np.log(cond_dist[label])\n",
        "            sub_log_prob_list.append(log_prob)\n",
        "        return sub_log_prob_list\n",
        "    # We break the ngram list into n_jobs sub-lists for parallel processing\n",
        "    # This is faster than parallel processing over each item\n",
        "    sub_ngrams_list_list = [ngrams_list[i::n_jobs] for i in range(n_jobs)]\n",
        "    log_prob_list_list = joblib.Parallel(n_jobs=n_jobs)(\n",
        "        joblib.delayed(func)(trie, sub_ngram_list)\n",
        "        for sub_ngram_list in sub_ngrams_list_list\n",
        "    )\n",
        "    log_prob_list = sorted([\n",
        "        log_prob \n",
        "        for sub_log_prob_list in log_prob_list_list\n",
        "        for log_prob in sub_log_prob_list\n",
        "    ])\n",
        "    count_based_ngram_perplexity = np.exp(-np.mean(log_prob_list))\n",
        "    return count_based_ngram_perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee3-MCH6rM8B"
      },
      "source": [
        "train_ngrams = get_ngram_dataset(train_tokens, tok2idx, n=4)\n",
        "val_ngrams = get_ngram_dataset(val_tokens, tok2idx, n=4)\n",
        "test_ngrams = get_ngram_dataset(test_tokens, tok2idx, n=4)\n",
        "\n",
        "trie_4gram = NgramTrie(n=4)\n",
        "for ngram in tqdm(train_ngrams):\n",
        "    trie_4gram.add_ngram(ngram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GmfefJflfNe"
      },
      "source": [
        "# This takes a while\n",
        "val_cbngram_perplexity = get_cbngram_perplexity(trie_4gram, val_ngrams)\n",
        "print(f\"Count-based 4-gram PPL: {val_cbngram_perplexity}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPVNsOpRoFQh"
      },
      "source": [
        "### Feedforward N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBwjV_Xmk5c"
      },
      "source": [
        "# Refer to Lab 3\n",
        "\n",
        "class NgramDataset(Dataset):\n",
        "   \n",
        "    def __init__(self, tokenized_arr):\n",
        "        self.tokenized_arr = tokenized_arr\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tokenized_arr.shape[0]\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        return self.tokenized_arr[key, :-1], self.tokenized_arr[key, -1]\n",
        "\n",
        "class NgramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, ngram_n, emb_dim):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.linear = nn.Linear(emb_dim * (ngram_n - 1), emb_dim)\n",
        "        self.out_embed = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "        # Share weights between embeddings\n",
        "        self.out_embed.weight = self.in_embed.weight\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "        out = self.in_embed(x)\n",
        "        out = out.reshape(batch_size, seq_len * self.emb_dim)\n",
        "        out = F.relu(self.linear(out))\n",
        "        out = self.out_embed(out)\n",
        "        return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def ffngram_eval_loop(model, dataloader):\n",
        "    loss_total = 0.0\n",
        "    count = 0\n",
        "    for batch_x, batch_y in tqdm(dataloader):\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
        "        preds = model(batch_x)\n",
        "        loss_sum = F.cross_entropy(preds, batch_y, reduction='sum')\n",
        "        loss_total += loss_sum.item()\n",
        "        count += batch_x.shape[0]\n",
        "    perplexity = np.exp(loss_total / count)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtIf_-WYoSu6"
      },
      "source": [
        "train_ffngram_dataset = NgramDataset(np.array(train_ngrams))\n",
        "val_ffngram_dataset = NgramDataset(np.array(val_ngrams))\n",
        "test_ffngram_dataset = NgramDataset(np.array(test_ngrams))\n",
        "\n",
        "train_ffngram_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_ffngram_dataset, \n",
        "    batch_size=8192,\n",
        "    shuffle=True,\n",
        ")\n",
        "val_ffngram_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=val_ffngram_dataset, \n",
        "    batch_size=8192,\n",
        ")\n",
        "test_ffngram_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=test_ffngram_dataset, \n",
        "    batch_size=8192,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6Q7SdzorYo"
      },
      "source": [
        "ffngram_model = NgramModel(vocab_size=len(vocab), ngram_n=4, emb_dim=200).to(DEVICE)\n",
        "ffngram_optimizer = torch.optim.Adam(ffngram_model.parameters(), lr=0.003)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuTgHVIqorc_"
      },
      "source": [
        "NUM_EPOCHS = 2\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "\n",
        "    # Train\n",
        "    for batch_x, batch_y in tqdm(train_ffngram_dataloader):\n",
        "        batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
        "        ffngram_optimizer.zero_grad()\n",
        "        preds = ffngram_model(batch_x)\n",
        "        loss = F.cross_entropy(preds, batch_y)\n",
        "        loss.backward()\n",
        "        ffngram_optimizer.step()\n",
        "    val_perplexity = ffngram_eval_loop(ffngram_model, val_ffngram_dataloader)\n",
        "    print(f\"Val perp: {val_perplexity}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WohXeXw1kJ4n"
      },
      "source": [
        "### KenLM N-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fi57EiFkLpC"
      },
      "source": [
        "! wget -O - https://kheafield.com/code/kenlm.tar.gz |tar xz\n",
        "! mkdir /content/kenlm/build; cd /content/kenlm/build; cmake ..; make -j 4\n",
        "! pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X53_-W6QkR_a"
      },
      "source": [
        "for phase in [\"train\", \"valid\", \"test\"]:\n",
        "    new_lines = []\n",
        "    with open(f\"ptb.{phase}.nounk.txt\", \"r\") as f_in:\n",
        "        for line in f_in:\n",
        "            if not line:\n",
        "                continue\n",
        "            truncated_line = \" \".join(line.strip().split()[:300])\n",
        "            new_lines.append(truncated_line)\n",
        "    with open(f\"ptb.{phase}.truncated.txt\", \"w\") as f_out:\n",
        "        f_out.write(\"\\n\".join(new_lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9plxNZol-Fi"
      },
      "source": [
        "# 4-gram\n",
        "!./kenlm/build/bin/lmplz -o 4 < ptb.train.truncated.txt > ptb_lm_4gram.arpa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh1sVSENnpjD"
      },
      "source": [
        "import kenlm\n",
        "kenlm_model = kenlm.LanguageModel('ptb_lm_4gram.arpa')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao-6r8PinvfF"
      },
      "source": [
        "def compute_kenlm_perplexity_on_file(kenlm_model, path):\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            numerator += -kenlm_model.score(line)\n",
        "            denominator += len(line.split())\n",
        "    return 10 ** (numerator / denominator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceTqIQU0o9Xv"
      },
      "source": [
        "compute_kenlm_perplexity_on_file(kenlm_model, \"ptb.valid.truncated.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8tdz8Nq__M"
      },
      "source": [
        "### Evaluate on our test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXgVHxhMoqQb"
      },
      "source": [
        "test_cbngram_perplexity = get_cbngram_perplexity(trie_4gram, test_ngrams)\n",
        "test_ffngram_perplexity = ffngram_eval_loop(ffngram_model, test_ffngram_dataloader)\n",
        "test_rnn_preplexity = compute_perplexity(eval_loop(\n",
        "    model=rnn_model,\n",
        "    dataloader=test_dataloader,\n",
        ")[\"loss\"])\n",
        "test_kenlm_perplexity = compute_kenlm_perplexity_on_file(\n",
        "    kenlm_model, \"ptb.test.truncated.txt\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4YxIcLhqIb0"
      },
      "source": [
        "print(\"Test Perplexities\")\n",
        "print()\n",
        "print(f\"Count-based 4-Gram:  {test_cbngram_perplexity}\")\n",
        "print(f\"FF 4-Gram:           {test_ffngram_perplexity}\")\n",
        "print(f\"KenLM 4-Gram:        {test_kenlm_perplexity}\")\n",
        "print(f\"RNN-LM:              {test_rnn_preplexity}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM2yqDXLpGnt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}